{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6262076,"sourceType":"datasetVersion","datasetId":3565867}],"dockerImageVersionId":30886,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-15T08:01:46.270301Z","iopub.execute_input":"2025-02-15T08:01:46.270720Z","iopub.status.idle":"2025-02-15T08:01:46.279953Z","shell.execute_reply.started":"2025-02-15T08:01:46.270687Z","shell.execute_reply":"2025-02-15T08:01:46.278780Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# About Dataset:\n- The Earthquakes-1990-2023 dataset contains historical earthquake records spanning over three decades, providing valuable insights into seismic activity worldwide. This dataset likely includes key attributes such as the date, time, latitude, longitude, depth, magnitude, and location of each earthquake. Additional fields may include tectonic plate information, event type, and tsunami warnings if sourced from organizations like USGS or EMSC. Analyzing this dataset can help identify geographic hotspots, temporal trends, and correlations between depth and magnitude. It can also be used to build machine learning models for earthquake prediction, employing classification techniques to determine the likelihood of significant tremors or regression models to estimate earthquake magnitudes based on historical data. Understanding these patterns is crucial for disaster preparedness, risk assessment, and early warning systems.","metadata":{}},{"cell_type":"markdown","source":"- The Earthquake Prediction project you've outlined involves analyzing a large dataset with over 3.4 million entries and 12 columns.\n\n - Columns:\n1. time: The timestamp of the earthquake event (int64).\n2. place: Location of the earthquake (object).\n3. status: Status of the earthquake event (object).\n4. tsunami: Indicator if a tsunami was triggered (int64).\n5. significance: Significance level of the earthquake (int64).\n6. data_type: Type of data recorded (object).\n7. magnitudo: Magnitude of the earthquake (float64).\n8. state: State where the earthquake occurred (object).\n9. longitude: Longitude of the earthquake (float64).\n10. latitude: Latitude of the earthquake (float64).\n11. depth: Depth of the earthquake in kilometers (float64).\n12. date: The date of the earthquake event (object). ","metadata":{}},{"cell_type":"markdown","source":"- Classification: If predicting the likelihood of an earthquake happening,  treat it as a classification problem (e.g., earthquake occurrence vs. non-occurrence).  use Random Forest, Decision Trees, or Logistic Regression & XGBoost.\n  ","metadata":{}},{"cell_type":"markdown","source":"# Import the libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom datetime import datetime","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T08:01:46.281428Z","iopub.execute_input":"2025-02-15T08:01:46.281715Z","iopub.status.idle":"2025-02-15T08:01:46.298339Z","shell.execute_reply.started":"2025-02-15T08:01:46.281693Z","shell.execute_reply":"2025-02-15T08:01:46.297151Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load the dataset","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/the-ultimate-earthquake-dataset-from-1990-2023/Eartquakes-1990-2023.csv\")\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T08:01:46.300258Z","iopub.execute_input":"2025-02-15T08:01:46.300643Z","iopub.status.idle":"2025-02-15T08:01:57.600239Z","shell.execute_reply.started":"2025-02-15T08:01:46.300606Z","shell.execute_reply":"2025-02-15T08:01:57.599026Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T08:01:57.602027Z","iopub.execute_input":"2025-02-15T08:01:57.602456Z","iopub.status.idle":"2025-02-15T08:01:57.608569Z","shell.execute_reply.started":"2025-02-15T08:01:57.602415Z","shell.execute_reply":"2025-02-15T08:01:57.607282Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T08:01:57.609759Z","iopub.execute_input":"2025-02-15T08:01:57.610144Z","iopub.status.idle":"2025-02-15T08:01:57.632248Z","shell.execute_reply.started":"2025-02-15T08:01:57.610109Z","shell.execute_reply":"2025-02-15T08:01:57.630998Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(df['date'].head(10))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T08:01:57.633383Z","iopub.execute_input":"2025-02-15T08:01:57.633678Z","iopub.status.idle":"2025-02-15T08:01:57.654616Z","shell.execute_reply.started":"2025-02-15T08:01:57.633654Z","shell.execute_reply":"2025-02-15T08:01:57.653294Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Convert Date and Time Columns","metadata":{}},{"cell_type":"code","source":"df['date'] = pd.to_datetime(df['date'], format='mixed', errors='coerce')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T08:01:57.657869Z","iopub.execute_input":"2025-02-15T08:01:57.658203Z","iopub.status.idle":"2025-02-15T08:02:03.481600Z","shell.execute_reply.started":"2025-02-15T08:01:57.658150Z","shell.execute_reply":"2025-02-15T08:02:03.480223Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['time'] = pd.to_datetime(df['time'], errors='coerce')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T08:02:03.483811Z","iopub.execute_input":"2025-02-15T08:02:03.484261Z","iopub.status.idle":"2025-02-15T08:02:04.971362Z","shell.execute_reply.started":"2025-02-15T08:02:03.484228Z","shell.execute_reply":"2025-02-15T08:02:04.970338Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Missing values","metadata":{}},{"cell_type":"code","source":"# Check for missing values\nprint(\"Missing Values Exists\")\nprint(df.isnull().sum())\n\n# Summary statistics\nprint(df.describe())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T08:02:04.972278Z","iopub.execute_input":"2025-02-15T08:02:04.972703Z","iopub.status.idle":"2025-02-15T08:02:06.638849Z","shell.execute_reply.started":"2025-02-15T08:02:04.972666Z","shell.execute_reply":"2025-02-15T08:02:06.637921Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['place'].fillna(\"Unknown\", inplace=True)\ndf['status'].fillna(df['status'].mode()[0], inplace=True)\ndf['state'].fillna(\"Unknown\", inplace=True)\ndf['data_type'].fillna(df['data_type'].mode()[0], inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T08:02:06.639747Z","iopub.execute_input":"2025-02-15T08:02:06.640024Z","iopub.status.idle":"2025-02-15T08:02:07.835520Z","shell.execute_reply.started":"2025-02-15T08:02:06.640004Z","shell.execute_reply":"2025-02-15T08:02:07.834333Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Label encoder","metadata":{}},{"cell_type":"code","source":"# Convert categorical variables to numerical using Label Encoding\nle = LabelEncoder()\ndf['place'] = le.fit_transform(df['place'])\ndf['status'] = le.fit_transform(df['status'])\ndf['state'] = le.fit_transform(df['state'])\ndf['data_type'] = le.fit_transform(df['data_type'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T08:02:07.836581Z","iopub.execute_input":"2025-02-15T08:02:07.836985Z","iopub.status.idle":"2025-02-15T08:02:14.381813Z","shell.execute_reply.started":"2025-02-15T08:02:07.836948Z","shell.execute_reply":"2025-02-15T08:02:14.380788Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T08:02:14.382831Z","iopub.execute_input":"2025-02-15T08:02:14.383138Z","iopub.status.idle":"2025-02-15T08:02:14.393769Z","shell.execute_reply.started":"2025-02-15T08:02:14.383112Z","shell.execute_reply":"2025-02-15T08:02:14.392417Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Remove outliers","metadata":{}},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\ndef plot_boxplots(df, columns):\n    for col in columns:\n        plt.figure(figsize=(12, 5))\n\n        # Subplot 1: Boxplot before removing outliers\n        plt.subplot(1, 2, 1)\n        sns.boxplot(x=df[col], color='red')\n        plt.title(f'Before Outlier Removal: {col}')\n\n        # IQR Calculation\n        Q1 = df[col].quantile(0.25)\n        Q3 = df[col].quantile(0.75)\n        IQR = Q3 - Q1\n        lower_bound = Q1 - 1.5 * IQR\n        upper_bound = Q3 + 1.5 * IQR\n\n        # Remove outliers (for visualization, not modifying df)\n        df_cleaned = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]\n\n        # Subplot 2: Boxplot after removing outliers\n        plt.subplot(1, 2, 2)\n        sns.boxplot(x=df_cleaned[col], color='green')\n        plt.title(f'After Outlier Removal: {col}')\n\n        plt.tight_layout()\n        plt.show()\n\n# List of numerical columns\nnumerical_columns = ['magnitudo', 'depth', 'longitude', 'latitude']\n\n# Plot boxplots before and after outlier removal\nplot_boxplots(df, numerical_columns)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T08:07:37.738996Z","iopub.execute_input":"2025-02-15T08:07:37.739353Z","iopub.status.idle":"2025-02-15T08:07:46.388523Z","shell.execute_reply.started":"2025-02-15T08:07:37.739326Z","shell.execute_reply":"2025-02-15T08:07:46.387556Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# observations:\n- function effectively visualizes outliers before and after removal using the IQR method. It calculates Q1, Q3, and the IQR to determine the lower and upper bounds for detecting outliers. Data points outside these bounds are considered outliers and removed for visualization in the second boxplot. The function does not modify the original DataFrame but creates a filtered version (df_cleaned) for plotting. Consider handling NaN values, checking for zero IQR cases (to avoid errors), and improving color contrast for better readability. This approach helps in understanding the impact of outlier removal on numerical features.","metadata":{}},{"cell_type":"markdown","source":"# Exploratory Data Analysis (EDA)","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T08:02:22.163429Z","iopub.execute_input":"2025-02-15T08:02:22.163755Z","iopub.status.idle":"2025-02-15T08:02:22.168684Z","shell.execute_reply.started":"2025-02-15T08:02:22.163729Z","shell.execute_reply":"2025-02-15T08:02:22.167146Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nsns.histplot(df['magnitudo'], kde=True, color='blue')\nplt.title('Distribution of Earthquake Magnitudes')\nplt.xlabel('Magnitude')\nplt.ylabel('Frequency')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T08:08:17.178291Z","iopub.execute_input":"2025-02-15T08:08:17.178671Z","iopub.status.idle":"2025-02-15T08:08:33.970653Z","shell.execute_reply.started":"2025-02-15T08:08:17.178643Z","shell.execute_reply":"2025-02-15T08:08:33.969369Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# observation:\n- This code creates a 10x6-inch figure displaying the distribution of earthquake magnitudes using a histogram with a Kernel Density Estimate (KDE) curve. It plots the data from the 'magnitudo' column in blue, providing a visual representation of the frequency of earthquake magnitudes. The plot includes a title (\"Distribution of Earthquake Magnitudes\") and axis labels (\"Magnitude\" for the x-axis and \"Frequency\" for the y-axis), enhancing clarity. Finally, plt.show() is called to display the plot. Ensure that you have imported matplotlib.pyplot and seaborn as plt and sns, respectively, for this to work properly.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12, 8))\nsns.heatmap(df.corr(), annot=True, cmap='coolwarm', linewidths=0.5)\nplt.title('Correlation Matrix')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T08:02:39.548192Z","iopub.execute_input":"2025-02-15T08:02:39.548566Z","iopub.status.idle":"2025-02-15T08:02:41.923895Z","shell.execute_reply.started":"2025-02-15T08:02:39.548532Z","shell.execute_reply":"2025-02-15T08:02:41.922538Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# observation:\n- The code generates a 12x8-inch heatmap of the correlation matrix for df dataset, using sns.heatmap(). It annotates each cell with correlation values, applies a 'coolwarm' color map, and sets linewidths for clarity. The plot is titled \"Correlation Matrix\" and displayed with plt.show(). Ensure you import matplotlib.pyplot and seaborn for the code to function correctly.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nsns.scatterplot(x='depth', y='magnitudo', data=df, color='red')\nplt.title('Depth vs Magnitude of Earthquakes')\nplt.xlabel('Depth (km)')\nplt.ylabel('Magnitude')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T08:02:41.925192Z","iopub.execute_input":"2025-02-15T08:02:41.925595Z","iopub.status.idle":"2025-02-15T08:02:49.093137Z","shell.execute_reply.started":"2025-02-15T08:02:41.925556Z","shell.execute_reply":"2025-02-15T08:02:49.092031Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# observation:\n- The code generates a scatter plot to visualize the relationship between the depth and magnitude of earthquakes. It creates a 10x6-inch figure, using red points to represent the data points where the x-axis is 'depth' (in kilometers) and the y-axis is 'magnitudo' (earthquake magnitude). The plot is titled \"Depth vs Magnitude of Earthquakes\" with labeled axes for clarity. plt.show() displays the plot. Ensure that you have imported the necessary libraries:","metadata":{}},{"cell_type":"markdown","source":"# Extract Time Features","metadata":{}},{"cell_type":"code","source":"df['year'] = df['date'].dt.year\ndf['month'] = df['date'].dt.month\ndf['day'] = df['date'].dt.day\ndf['hour'] = df['time'].dt.hour","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T08:02:49.097521Z","iopub.execute_input":"2025-02-15T08:02:49.097883Z","iopub.status.idle":"2025-02-15T08:02:49.450242Z","shell.execute_reply.started":"2025-02-15T08:02:49.097852Z","shell.execute_reply":"2025-02-15T08:02:49.448637Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['day_of_week'] = df['date'].dt.weekday","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T08:02:49.452194Z","iopub.execute_input":"2025-02-15T08:02:49.452607Z","iopub.status.idle":"2025-02-15T08:02:49.571071Z","shell.execute_reply.started":"2025-02-15T08:02:49.452565Z","shell.execute_reply":"2025-02-15T08:02:49.569954Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['season'] = df['month'].apply(lambda x: 'Winter' if x in [12, 1, 2] else \n                                            'Spring' if x in [3, 4, 5] else \n                                           'Summer' if x in [6, 7, 8] else 'Fall')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T08:02:49.571981Z","iopub.execute_input":"2025-02-15T08:02:49.572304Z","iopub.status.idle":"2025-02-15T08:02:50.420126Z","shell.execute_reply.started":"2025-02-15T08:02:49.572280Z","shell.execute_reply":"2025-02-15T08:02:50.418714Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# standardize the dataset","metadata":{}},{"cell_type":"code","source":"scaler = StandardScaler()\n\nnumerical_features = ['magnitudo', 'depth', 'longitude', 'latitude']\ndf[numerical_features] = scaler.fit_transform(df[numerical_features])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T08:02:50.421902Z","iopub.execute_input":"2025-02-15T08:02:50.422423Z","iopub.status.idle":"2025-02-15T08:02:50.716857Z","shell.execute_reply.started":"2025-02-15T08:02:50.422387Z","shell.execute_reply":"2025-02-15T08:02:50.715754Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df['high_magnitude'] = np.where(df['magnitudo'] > 5, 1, 0)  # Example: use 5 as the threshold","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T08:02:50.718000Z","iopub.execute_input":"2025-02-15T08:02:50.718326Z","iopub.status.idle":"2025-02-15T08:02:50.750004Z","shell.execute_reply.started":"2025-02-15T08:02:50.718301Z","shell.execute_reply":"2025-02-15T08:02:50.748911Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create target variable 'high_magnitude' (1 if magnitude > 6, else 0)\n#df['high_magnitude'] = np.where(df['magnitudo'] > 6, 1, 0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T08:02:50.750965Z","iopub.execute_input":"2025-02-15T08:02:50.751278Z","iopub.status.idle":"2025-02-15T08:02:50.755678Z","shell.execute_reply.started":"2025-02-15T08:02:50.751254Z","shell.execute_reply":"2025-02-15T08:02:50.754287Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# feature and target","metadata":{}},{"cell_type":"code","source":"# Features and target\nX = df.drop(['high_magnitude', 'date', 'time'], axis=1)\ny = df['high_magnitude']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T08:09:05.493465Z","iopub.execute_input":"2025-02-15T08:09:05.493825Z","iopub.status.idle":"2025-02-15T08:09:05.731503Z","shell.execute_reply.started":"2025-02-15T08:09:05.493786Z","shell.execute_reply":"2025-02-15T08:09:05.730623Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(df['high_magnitude'].value_counts())  # Check if both 0s and 1s exist\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T08:02:51.028113Z","iopub.execute_input":"2025-02-15T08:02:51.028519Z","iopub.status.idle":"2025-02-15T08:02:51.056617Z","shell.execute_reply.started":"2025-02-15T08:02:51.028479Z","shell.execute_reply":"2025-02-15T08:02:51.055715Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(df['magnitudo'].isnull().sum())  # Check for missing values","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T08:02:51.057573Z","iopub.execute_input":"2025-02-15T08:02:51.057895Z","iopub.status.idle":"2025-02-15T08:02:51.088365Z","shell.execute_reply.started":"2025-02-15T08:02:51.057870Z","shell.execute_reply":"2025-02-15T08:02:51.087381Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# train test split","metadata":{}},{"cell_type":"code","source":"#df = df.sample(frac=1, random_state=42).reset_index(drop=True)  # Shuffle and reset index","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T08:02:51.089327Z","iopub.execute_input":"2025-02-15T08:02:51.089612Z","iopub.status.idle":"2025-02-15T08:02:51.094061Z","shell.execute_reply.started":"2025-02-15T08:02:51.089588Z","shell.execute_reply":"2025-02-15T08:02:51.092545Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T08:02:51.095425Z","iopub.execute_input":"2025-02-15T08:02:51.095851Z","iopub.status.idle":"2025-02-15T08:02:54.017758Z","shell.execute_reply.started":"2025-02-15T08:02:51.095803Z","shell.execute_reply":"2025-02-15T08:02:54.016792Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T08:02:54.018706Z","iopub.execute_input":"2025-02-15T08:02:54.019010Z","iopub.status.idle":"2025-02-15T08:02:54.022912Z","shell.execute_reply.started":"2025-02-15T08:02:54.018977Z","shell.execute_reply":"2025-02-15T08:02:54.021890Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## One-Hot Encoding","metadata":{}},{"cell_type":"code","source":"X_train = pd.get_dummies(X_train, drop_first=True)\nX_test = pd.get_dummies(X_test, drop_first=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T08:02:54.023793Z","iopub.execute_input":"2025-02-15T08:02:54.024266Z","iopub.status.idle":"2025-02-15T08:02:54.913887Z","shell.execute_reply.started":"2025-02-15T08:02:54.024240Z","shell.execute_reply":"2025-02-15T08:02:54.912765Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Classification Models","metadata":{}},{"cell_type":"code","source":"# Initialize and train the model\nrf_model = RandomForestClassifier(n_estimators=100, random_state=42)\nrf_model.fit(X_train, y_train)\n\n# Predictions\ny_pred = rf_model.predict(X_test)\n\n# Evaluate the model's performance\nprint(\"Classification Report:\")\nprint(classification_report(y_test, y_pred))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T08:02:54.914773Z","iopub.execute_input":"2025-02-15T08:02:54.915049Z","iopub.status.idle":"2025-02-15T08:05:02.245889Z","shell.execute_reply.started":"2025-02-15T08:02:54.915028Z","shell.execute_reply":"2025-02-15T08:05:02.245004Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\n# Initialize and train Decision Tree model\ndt_model = DecisionTreeClassifier(random_state=42)\ndt_model.fit(X_train, y_train)\n\n# Predictions and evaluation\ny_pred_dt = dt_model.predict(X_test)\nprint(\"Decision Tree Classification Report:\")\nprint(classification_report(y_test, y_pred_dt))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T08:05:02.247051Z","iopub.execute_input":"2025-02-15T08:05:02.247369Z","iopub.status.idle":"2025-02-15T08:05:07.008515Z","shell.execute_reply.started":"2025-02-15T08:05:02.247344Z","shell.execute_reply":"2025-02-15T08:05:07.007490Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from xgboost import XGBClassifier  \n\nxgb = XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42)  \nxgb.fit(X_train, y_train)  \nprint(classification_report(y_test, xgb.predict(X_test)))  ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T08:05:07.009518Z","iopub.execute_input":"2025-02-15T08:05:07.009795Z","iopub.status.idle":"2025-02-15T08:05:21.774731Z","shell.execute_reply.started":"2025-02-15T08:05:07.009773Z","shell.execute_reply":"2025-02-15T08:05:21.773652Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n\ndef evaluate_model(model, X_test, y_test, model_name):\n    y_pred = model.predict(X_test)\n    \n    # Check if the model has predict_proba() and if y_test has both classes (0 and 1)\n    if hasattr(model, \"predict_proba\") and len(set(y_test)) > 1:\n        y_proba = model.predict_proba(X_test)[:, 1]  # Extract probability of class 1\n        roc_auc = roc_auc_score(y_test, y_proba)\n    else:\n        y_proba = None\n        roc_auc = \"N/A (Only one class present in y_test)\"\n    \n    print(f\"\\n===== {model_name} Evaluation =====\")\n    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n    print(f\"Precision: {precision_score(y_test, y_pred, zero_division=1):.4f}\")\n    print(f\"Recall: {recall_score(y_test, y_pred, zero_division=1):.4f}\")\n    print(f\"F1 Score: {f1_score(y_test, y_pred, zero_division=1):.4f}\")\n\n# Evaluate models\nevaluate_model(rf_model, X_test, y_test, \"Random Forest\")\nevaluate_model(dt_model, X_test, y_test, \"Decision Tree\")\nevaluate_model(xgb, X_test, y_test, \"XGBoost\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-15T08:05:21.775868Z","iopub.execute_input":"2025-02-15T08:05:21.776289Z","iopub.status.idle":"2025-02-15T08:05:38.872641Z","shell.execute_reply.started":"2025-02-15T08:05:21.776251Z","shell.execute_reply":"2025-02-15T08:05:38.871494Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- The evaluation results indicate that the Random Forest and Decision Tree models are performing perfectly with an accuracy, precision, recall, and F1 score of 1.0000, which suggests that they are classifying the data correctly in both classes.\n\n- However, the XGBoost model is showing perfect accuracy and precision (1.0000) but has a recall and F1 score of 0.0000. This suggests that while it is predicting the positive class (high_magnitude = 1) correctly in some instances, it is failing to identify any true positives in the test set (recall = 0).","metadata":{}},{"cell_type":"markdown","source":"# Conclusion:\n\n- The Earthquake Prediction project utilized three machine learning models—Random Forest, Decision Tree, and XGBoost—to predict the occurrence of significant earthquakes based on various features such as magnitude, depth, and location. Both the Random Forest and Decision Tree models delivered outstanding performance, achieving perfect accuracy, precision, recall, and F1 scores of 1.0000, indicating that they accurately classified earthquake occurrences and non-occurrences. The XGBoost model also demonstrated excellent accuracy and precision but showed room for improvement in recall and F1 score. Overall, the Random Forest and Decision Tree models proved highly effective for the task, with strong predictive capabilities. This suggests that these models could be relied upon for accurate earthquake prediction, while further exploration of the XGBoost model may yield valuable insights.","metadata":{}}]}